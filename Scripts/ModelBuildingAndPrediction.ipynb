{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv, datetime\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import fastai as fast\n",
    "from sklearn.metrics import *\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "from sklearn.model_selection import GridSearchCV   #Performing grid search\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Scoring Function Based on combined adjusted AUC and adjusted Kappa\n",
    "def scoreFunc(y_test, preds):\n",
    "    rocAUC = roc_auc_score(y_test, preds)\n",
    "    print('AUC: ' + str(rocAUC))\n",
    "    roundedPreds = preds.round()\n",
    "    Kappa = cohen_kappa_score(y_test,roundedPreds)\n",
    "\n",
    "    AdjAUC = (rocAUC - 0.5) * 2 if (rocAUC>0.5) else 0\n",
    "    print('AdjustedAUC: ' + str(AdjAUC))\n",
    "    AdjKappa = Kappa if (Kappa > 0) else 0\n",
    "    print('Kappa: ' + str(Kappa))\n",
    "    print('AdjustedKappa: ' + str(AdjKappa))\n",
    "    return AdjAUC + AdjKappa\n",
    "\n",
    "#Scoring Function Based on combined adjusted AUC and adjusted Kappa\n",
    "def evalFunc(preds, y_test):\n",
    "    y_labels = y_test.get_label()\n",
    "    rocAUC = roc_auc_score(y_labels, preds)\n",
    "    roundedPreds = preds.round()\n",
    "    Kappa = cohen_kappa_score(y_labels,roundedPreds)\n",
    "    AdjAUC = (rocAUC - 0.5) * 2 if (rocAUC>0.5) else 0\n",
    "    AdjKappa = Kappa if (Kappa > 0) else 0\n",
    "    return 'rocAucKappa',float(1 - (AdjAUC + AdjKappa))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams.update({'font.size':12, 'figure.figsize':[10,7]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in Training Feature Data\n",
    "train = pd.read_csv('../TrainingData/FullScrapeT10.csv')\n",
    "target = 'Target'\n",
    "IDcol = 'StudentID'\n",
    "train = train.drop(['StudentID'],axis=1)\n",
    "#remove duplicates\n",
    "train = train.loc[:,~train.columns.str.endswith('.1')]\n",
    "\n",
    "#Read in Hidden Feature Data\n",
    "dataPred = pd.read_csv('../TrainingData/FullScrapeH10.csv')\n",
    "idDF = pd.DataFrame(dataPred.StudentID)\n",
    "\n",
    "#Convert from boolean to binary\n",
    "train[target] *=1\n",
    "train.head()\n",
    "\n",
    "#Data Cleaning - Optional\n",
    "\n",
    "#Remove more than threshold missing values\n",
    "print(\"Removing columns with less than 10 values\")\n",
    "threshold = 10\n",
    "train = train.dropna(axis=1,thresh=threshold)\n",
    "train.head()\n",
    "\n",
    "#In case Hidden and Training have different columns\n",
    "#This is primarily for click rate for which additional columns are generated for every 10 clicks.\n",
    "print(\"Removing columns not present in both training and hidden\")\n",
    "for col in train.columns:\n",
    "    if (not col in dataPred.columns and not col==target):\n",
    "        train = train.drop(col, axis=1)\n",
    "        print(i)\n",
    "        \n",
    "#Seperate Target and Features\n",
    "X, y = train.drop([target],axis=1),train[target]\n",
    "#Sort X columns alphabetically\n",
    "X = X.reindex(sorted(X.columns), axis=1)\n",
    "\n",
    "#Split into training and validation set\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=123)\n",
    "\n",
    "\n",
    "#XGBRegressor Model\n",
    "xg_reg = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "scorer = make_scorer(scoreFunc)\n",
    "#kfold = StratifiedKFold(10,shuffle=True,random_state=1988)\n",
    "parameters = {\n",
    "    'objective':['binary:logistic'],\n",
    "    'learning_rate':np.linspace(.01,.1,num=10,endpoint=True),\n",
    "    'max_depth': range(3,8,1),\n",
    "    'min_child_weight': range(1,7,1),\n",
    "    'subsample': np.linspace(.1,1.0,num=10,endpoint=True),\n",
    "    'colsample_bytree': np.linspace(.1,1.0,num=10,endpoint=True),\n",
    "    'gamma':range(0,5,1),\n",
    "    'n_estimators': [100]}\n",
    "\n",
    "while(len(X.columns) > 30):\n",
    "    xgb_grid = RandomizedSearchCV(xg_reg,  parameters,n_iter=20,n_jobs = -1,scoring=scorer, cv=10, verbose=True)\n",
    "\n",
    "    xgb_grid.fit(X, y)\n",
    "\n",
    "    best_model = xgb_grid.best_estimator_\n",
    "\n",
    "    print(xgb_grid.best_score_)\n",
    "    print(xgb_grid.best_params_)\n",
    "    \n",
    "    #Calculate important features based on number of trees in which they occur\n",
    "    featImportArr = best_model.feature_importances_\n",
    "    fScores = best_model.get_booster().get_fscore()\n",
    "    fScores = fScores.items()\n",
    "    fScores = sorted(fScores, key=lambda x: x[1])\n",
    "    fScores = fScores[int(len(fScores)*0.25):]\n",
    "    keptFeatures = [i[0] for i in fScores]\n",
    "\n",
    "    #Only keep top 75% of features\n",
    "    print(\"Removing columns based on feature importance\")\n",
    "    for col in train.columns:\n",
    "        if (not col in keptFeatures and not col==target):\n",
    "            train = train.drop(col, axis=1)\n",
    "            print(col)\n",
    "\n",
    "    #Seperate Target and Features\n",
    "    X, y = train.drop([target],axis=1),train[target]\n",
    "    print(\"Number of Features Remaining: \", len(X.columns))\n",
    "    #Sort X columns alphabetically\n",
    "    X = X.reindex(sorted(X.columns), axis=1)\n",
    "    print(\"Number of Features Remaining: \", len(X.columns))\n",
    "\n",
    "    #XGBRegressor Model\n",
    "    xg_reg = xgb.XGBRegressor()\n",
    "    \n",
    "xgb_grid = RandomizedSearchCV(xg_reg,  parameters,n_iter=200,n_jobs = -1,scoring=scorer, cv=10, verbose=True)\n",
    "\n",
    "xgb_grid.fit(X, y)\n",
    "\n",
    "best_model = xgb_grid.best_estimator_\n",
    "\n",
    "best_model = xgb_grid.best_estimator_\n",
    "\n",
    "print(xgb_grid.best_score_)\n",
    "print(xgb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Hyper-parameters and Validation Score\n",
    "#0.6442671192414215\n",
    "#{'subsample': 0.9, 'objective': 'binary:logistic', 'n_estimators': 90, \n",
    "# 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.07, 'gamma': 0, 'colsample_bytree': 0.8}\n",
    "\n",
    "#OPTIONAL- Use Grid Search for more comprehensive hyperparameter tuning\n",
    "#from sklearn.metrics import make_scorer\n",
    "#from sklearn.model_selection import StratifiedKFold\n",
    "#from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#scorer = make_scorer(scoreFunc)\n",
    "#kfold = StratifiedKFold(10,shuffle=True,random_state=1988)\n",
    "#parameters = {\n",
    "#    'objective':['binary:logistic'],\n",
    "#    'learning_rate':[.03],\n",
    "#    'max_depth': [8],\n",
    "#    'min_child_weight': [3],\n",
    "#    'subsample': [0.7],\n",
    "#    'colsample_bytree': [0.4],\n",
    "#    'gamma':[6],\n",
    "#    'n_estimators': range(190,210,2)}\n",
    "\n",
    "#xgb_grid = GridSearchCV(xg_reg,  parameters,n_jobs = -1,scoring=scorer, cv=3, verbose=True)\n",
    "\n",
    "#xgb_grid.fit(X_train,y_train)\n",
    "\n",
    "#print(xgb_grid.best_score_)\n",
    "#print(xgb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xgb.plot_importance(best_model,max_num_features=10, title = \"Feature Importance 10 Minute Model\")\n",
    "plt.rcParams['figure.figsize'] = [10, 7]\n",
    "plt.savefig('FeatImport_10.png')\n",
    "\n",
    "\n",
    "for col in dataPred.columns:\n",
    "    if (not col in train.columns):\n",
    "        dataPred = dataPred.drop(col, axis=1)\n",
    "        \n",
    "#dataPred= dataPred.drop('Target',axis=1)\n",
    "dataPred = dataPred.reindex(sorted(dataPred.columns), axis=1)\n",
    "print(dataPred.head())\n",
    "\n",
    "preds = best_model.predict(dataPred)\n",
    "\n",
    "predDF = pd.DataFrame(preds,columns=['Target'])\n",
    "predDF.head()\n",
    "output = pd.concat([idDF,predDF], axis = 1)\n",
    "output.head()\n",
    "\n",
    "output.to_csv('hidden10_Output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#20 Minute Model\n",
    "#Read in Training Feature Data\n",
    "train = pd.read_csv('../TrainingData/FullScrapeT20.csv')\n",
    "target = 'Target'\n",
    "IDcol = 'StudentID'\n",
    "train = train.drop(['StudentID'],axis=1)\n",
    "#remove duplicates\n",
    "train = train.loc[:,~train.columns.str.endswith('.1')]\n",
    "\n",
    "#Read in Hidden Feature Data\n",
    "dataPred = pd.read_csv('../TrainingData/FullScrapeH20.csv')\n",
    "idDF = pd.DataFrame(dataPred.StudentID)\n",
    "\n",
    "#Convert from boolean to binary\n",
    "train[target] *=1\n",
    "train.head()\n",
    "\n",
    "#Data Cleaning - Optional\n",
    "\n",
    "#Remove more than threshold missing values\n",
    "print(\"Removing columns with less than 10 values\")\n",
    "threshold = 10\n",
    "train = train.dropna(axis=1,thresh=threshold)\n",
    "train.head()\n",
    "\n",
    "#In case Hidden and Training have different columns\n",
    "#This is primarily for click rate for which additional columns are generated for every 10 clicks.\n",
    "print(\"Removing columns not present in both training and hidden\")\n",
    "for col in train.columns:\n",
    "    if (not col in dataPred.columns and not col==target):\n",
    "        train = train.drop(col, axis=1)\n",
    "        print(i)\n",
    "        \n",
    "#Seperate Target and Features\n",
    "X, y = train.drop([target],axis=1),train[target]\n",
    "#Sort X columns alphabetically\n",
    "X = X.reindex(sorted(X.columns), axis=1)\n",
    "\n",
    "#Split into training and validation set\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=123)\n",
    "\n",
    "#XGBRegressor Model\n",
    "xg_reg = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = make_scorer(scoreFunc)\n",
    "#kfold = StratifiedKFold(10,shuffle=True,random_state=1988)\n",
    "parameters = {\n",
    "    'objective':['binary:logistic'],\n",
    "    'learning_rate':np.linspace(.01,.1,num=10,endpoint=True),\n",
    "    'max_depth': range(3,8,1),\n",
    "    'min_child_weight': range(1,7,1),\n",
    "    'subsample': np.linspace(.1,1.0,num=10,endpoint=True),\n",
    "    'colsample_bytree': np.linspace(.1,1.0,num=10,endpoint=True),\n",
    "    'gamma':range(0,5,1),\n",
    "    'n_estimators': [100]}\n",
    "\n",
    "while(len(X.columns) > 30):\n",
    "    xgb_grid = RandomizedSearchCV(xg_reg,  parameters,n_iter=20,n_jobs = -1,scoring=scorer, cv=10, verbose=True)\n",
    "\n",
    "    xgb_grid.fit(X, y)\n",
    "\n",
    "    best_model = xgb_grid.best_estimator_\n",
    "\n",
    "    print(xgb_grid.best_score_)\n",
    "    print(xgb_grid.best_params_)\n",
    "\n",
    "    featImportArr = best_model.feature_importances_\n",
    "    fScores = best_model.get_booster().get_fscore()\n",
    "    fScores = fScores.items()\n",
    "    fScores = sorted(fScores, key=lambda x: x[1])\n",
    "\n",
    "    fScores = fScores[int(len(fScores)*0.25):]\n",
    "    keptFeatures = [i[0] for i in fScores]\n",
    "    keptFeatures\n",
    "\n",
    "    #Only keep top 75% of features\n",
    "    print(\"Removing columns based on feature importance\")\n",
    "    for col in train.columns:\n",
    "        if (not col in keptFeatures and not col==target):\n",
    "            train = train.drop(col, axis=1)\n",
    "            print(col)\n",
    "\n",
    "    #Seperate Target and Features\n",
    "    X, y = train.drop([target],axis=1),train[target]\n",
    "    print(\"Number of Features Remaining: \", len(X.columns))\n",
    "    #Sort X columns alphabetically\n",
    "    X = X.reindex(sorted(X.columns), axis=1)\n",
    "\n",
    "    #XGBRegressor Model\n",
    "    xg_reg = xgb.XGBRegressor(importance_type = 'weight')\n",
    "    \n",
    "xgb_grid = RandomizedSearchCV(xg_reg,  parameters,n_iter=200,n_jobs = -1,scoring=scorer, cv=10, verbose=True)\n",
    "\n",
    "xgb_grid.fit(X, y)\n",
    "\n",
    "best_model = xgb_grid.best_estimator_\n",
    "\n",
    "best_model = xgb_grid.best_estimator_\n",
    "\n",
    "print(xgb_grid.best_score_)\n",
    "print(xgb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Hyper-parameters and Validation Score\n",
    "#0.7593081688312175\n",
    "#{'subsample': 1.0, 'objective': 'binary:logistic', 'n_estimators': 70, 'min_child_weight': 3,\n",
    "# 'max_depth': 4, 'learning_rate': 0.05000000000000001, 'gamma': 3, 'colsample_bytree': 0.30000000000000004}\n",
    "\n",
    "#OPTIONAL- Use Grid Search for more comprehensive hyperparameter tuning\n",
    "#from sklearn.metrics import make_scorer\n",
    "#from sklearn.model_selection import StratifiedKFold\n",
    "#from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#scorer = make_scorer(scoreFunc)\n",
    "#kfold = StratifiedKFold(10,shuffle=True,random_state=1988)\n",
    "#parameters = {\n",
    "#    'objective':['binary:logistic'],\n",
    "#    'learning_rate':[.03],\n",
    "#    'max_depth': [8],\n",
    "#    'min_child_weight': [3],\n",
    "#    'subsample': [0.7],\n",
    "#    'colsample_bytree': [0.4],\n",
    "#    'gamma':[6],\n",
    "#    'n_estimators': range(190,210,2)}\n",
    "\n",
    "#xgb_grid = GridSearchCV(xg_reg,  parameters,n_jobs = -1,scoring=scorer, cv=3, verbose=True)\n",
    "\n",
    "#xgb_grid.fit(X_train,y_train)\n",
    "\n",
    "#print(xgb_grid.best_score_)\n",
    "#print(xgb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xgb.plot_importance(best_model,max_num_features=10, title=\"Feature Importance 20 Minute Model\")\n",
    "plt.savefig('FeatImport_20.png')\n",
    "\n",
    "\n",
    "for col in dataPred.columns:\n",
    "    if (not col in train.columns):\n",
    "        dataPred = dataPred.drop(col, axis=1)\n",
    "        \n",
    "#dataPred= dataPred.drop('Target',axis=1)\n",
    "dataPred = dataPred.reindex(sorted(dataPred.columns), axis=1)\n",
    "print(dataPred.head())\n",
    "\n",
    "preds = best_model.predict(dataPred)\n",
    "\n",
    "predDF = pd.DataFrame(preds,columns=['Target'])\n",
    "predDF.head()\n",
    "output = pd.concat([idDF,predDF], axis = 1)\n",
    "output.head()\n",
    "\n",
    "output.to_csv('hidden20_Output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#30 Minute Model\n",
    "#Read in Training Feature Data\n",
    "train = pd.read_csv('../TrainingData/FullScrapeT30.csv')\n",
    "target = 'Target'\n",
    "IDcol = 'StudentID'\n",
    "train = train.drop(['StudentID'],axis=1)\n",
    "#remove duplicates\n",
    "train = train.loc[:,~train.columns.str.endswith('.1')]\n",
    "\n",
    "#Read in Hidden Feature Data\n",
    "dataPred = pd.read_csv('../TrainingData/FullScrapeH30.csv')\n",
    "idDF = pd.DataFrame(dataPred.StudentID)\n",
    "\n",
    "\n",
    "#Convert from boolean to binary\n",
    "train[target] *=1\n",
    "train.head()\n",
    "\n",
    "#Data Cleaning - Optional\n",
    "\n",
    "#Remove more than threshold missing values\n",
    "print(\"Removing columns with less than 10 values\")\n",
    "threshold = 10\n",
    "train = train.dropna(axis=1,thresh=threshold)\n",
    "train.head()\n",
    "\n",
    "#In case Hidden and Training have different columns\n",
    "#This is primarily for click rate for which additional columns are generated for every 10 clicks.\n",
    "print(\"Removing columns not present in both training and hidden\")\n",
    "for col in train.columns:\n",
    "    if (not col in dataPred.columns and not col==target):\n",
    "        train = train.drop(col, axis=1)\n",
    "        print(i)\n",
    "        \n",
    "#Seperate Target and Features\n",
    "X, y = train.drop([target],axis=1),train[target]\n",
    "#Sort X columns alphabetically\n",
    "X = X.reindex(sorted(X.columns), axis=1)\n",
    "\n",
    "#Split into training and validation set\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=123)\n",
    "\n",
    "#XGBRegressor Model\n",
    "xg_reg = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = make_scorer(scoreFunc)\n",
    "#kfold = StratifiedKFold(10,shuffle=True,random_state=1988)\n",
    "parameters = {\n",
    "    'objective':['binary:logistic'],\n",
    "    'learning_rate':np.linspace(.01,.1,num=10,endpoint=True),\n",
    "    'max_depth': range(3,8,1),\n",
    "    'min_child_weight': range(1,7,1),\n",
    "    'subsample': np.linspace(.1,1.0,num=10,endpoint=True),\n",
    "    'colsample_bytree': np.linspace(.1,1.0,num=10,endpoint=True),\n",
    "    'gamma':range(0,5,1),\n",
    "    'n_estimators': [100]}\n",
    "\n",
    "while(len(X.columns) > 30):\n",
    "    xgb_grid = RandomizedSearchCV(xg_reg,  parameters,n_iter=20,n_jobs = -1,scoring=scorer, cv=10, verbose=True)\n",
    "\n",
    "    xgb_grid.fit(X, y)\n",
    "\n",
    "    best_model = xgb_grid.best_estimator_\n",
    "\n",
    "    print(xgb_grid.best_score_)\n",
    "    print(xgb_grid.best_params_)\n",
    "\n",
    "    featImportArr = best_model.feature_importances_\n",
    "    fScores = best_model.get_booster().get_fscore()\n",
    "    fScores = fScores.items()\n",
    "    fScores = sorted(fScores, key=lambda x: x[1])\n",
    "\n",
    "    fScores = fScores[int(len(fScores)*0.25):]\n",
    "    keptFeatures = [i[0] for i in fScores]\n",
    "    keptFeatures\n",
    "\n",
    "    #Only keep top 75% of features.\n",
    "    print(\"Removing columns based on feature importance\")\n",
    "    for col in train.columns:\n",
    "        if (not col in keptFeatures and not col==target):\n",
    "            train = train.drop(col, axis=1)\n",
    "            print(col)\n",
    "    #Seperate Target and Features\n",
    "    X, y = train.drop([target],axis=1),train[target]\n",
    "    print(\"Number of Features Remaining: \", len(X.columns))\n",
    "    #Sort X columns alphabetically\n",
    "    X = X.reindex(sorted(X.columns), axis=1)\n",
    "\n",
    "    #XGBRegressor Model\n",
    "    xg_reg = xgb.XGBRegressor(importance_type = 'weight')\n",
    "    \n",
    "xgb_grid = RandomizedSearchCV(xg_reg,  parameters,n_iter=200,n_jobs = -1,scoring=scorer, cv=10, verbose=True)\n",
    "\n",
    "xgb_grid.fit(X, y)\n",
    "\n",
    "best_model = xgb_grid.best_estimator_\n",
    "\n",
    "best_model = xgb_grid.best_estimator_\n",
    "\n",
    "print(xgb_grid.best_score_)\n",
    "print(xgb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Hyper-parameters and Validation Score\n",
    "#0.7842701038048573\n",
    "#{'subsample': 0.6, 'objective': 'binary:logistic', 'n_estimators': 90, 'min_child_weight': 5,\n",
    "# 'max_depth': 3, 'learning_rate': 0.06000000000000001, 'gamma': 2, 'colsample_bytree': 0.30000000000000004}\n",
    "\n",
    "#OPTIONAL- Use Grid Search for more comprehensive hyperparameter tuning\n",
    "#from sklearn.metrics import make_scorer\n",
    "#from sklearn.model_selection import StratifiedKFold\n",
    "#from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#scorer = make_scorer(scoreFunc)\n",
    "#kfold = StratifiedKFold(10,shuffle=True,random_state=1988)\n",
    "#parameters = {\n",
    "#    'objective':['binary:logistic'],\n",
    "#    'learning_rate':[.03],\n",
    "#    'max_depth': [8],\n",
    "#    'min_child_weight': [3],\n",
    "#    'subsample': [0.7],\n",
    "#    'colsample_bytree': [0.4],\n",
    "#    'gamma':[6],\n",
    "#    'n_estimators': range(190,210,2)}\n",
    "\n",
    "#xgb_grid = GridSearchCV(xg_reg,  parameters,n_jobs = -1,scoring=scorer, cv=3, verbose=True)\n",
    "\n",
    "#xgb_grid.fit(X_train,y_train)\n",
    "\n",
    "#print(xgb_grid.best_score_)\n",
    "#print(xgb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xgb.plot_importance(best_model,max_num_features=10,title = \"Feature Importance 30 Minute Model\")\n",
    "plt.rcParams['figure.figsize'] = [10, 7]\n",
    "plt.savefig('FeatImport_30.png')\n",
    "\n",
    "\n",
    "for col in dataPred.columns:\n",
    "    if (not col in train.columns):\n",
    "        dataPred = dataPred.drop(col, axis=1)\n",
    "        \n",
    "#dataPred= dataPred.drop('Target',axis=1)\n",
    "dataPred = dataPred.reindex(sorted(dataPred.columns), axis=1)\n",
    "print(dataPred.head())\n",
    "\n",
    "preds = best_model.predict(dataPred)\n",
    "\n",
    "predDF = pd.DataFrame(preds,columns=['Target'])\n",
    "predDF.head()\n",
    "output = pd.concat([idDF,predDF], axis = 1)\n",
    "output.head()\n",
    "\n",
    "output.to_csv('hidden30_Output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in predictions and combine them\n",
    "hidden10 = pd.read_csv('hidden10_Output.csv')\n",
    "hidden20 = pd.read_csv('hidden20_Output.csv')\n",
    "hidden30 = pd.read_csv('hidden30_Output.csv')\n",
    "frames = [hidden10,hidden20,hidden30]\n",
    "result = pd.concat(frames)\n",
    "\n",
    "#Sort by StudentID to match test\n",
    "hidden10 = hidden10.sort_values(by='StudentID')\n",
    "hidden20 = hidden20.sort_values(by='StudentID')\n",
    "hidden30 = hidden30.sort_values(by='StudentID')\n",
    "result = result.sort_values(by=['StudentID'])\n",
    "\n",
    "\n",
    "list_10_ids = hidden10['StudentID'].to_list()\n",
    "list_20_ids = hidden20['StudentID'].to_list()\n",
    "list_30_ids = hidden30['StudentID'].to_list()\n",
    "\n",
    "################################################################################################################################\n",
    "#Public Leaderboard Scoring\n",
    "\n",
    "#Read in public leaderboard test\n",
    "publicTargets = pd.read_csv('../EvaluationData/hidden_leaderboard.csv')\n",
    "\n",
    "#filter for public leaderboard ids\n",
    "list_of_ids = publicTargets['STUDENTID'].to_list()\n",
    "\n",
    "public10Preds = hidden10[hidden10.StudentID.isin(list_of_ids)]\n",
    "public10Targets = publicTargets[publicTargets.STUDENTID.isin(list_10_ids)]\n",
    "\n",
    "public20Preds = hidden20[hidden20.StudentID.isin(list_of_ids)]\n",
    "public20Targets = publicTargets[publicTargets.STUDENTID.isin(list_20_ids)]\n",
    "\n",
    "public30Preds = hidden30[hidden30.StudentID.isin(list_of_ids)]\n",
    "public30Targets = publicTargets[publicTargets.STUDENTID.isin(list_30_ids)]\n",
    "\n",
    "publicPreds = result[result.StudentID.isin(list_of_ids)]\n",
    "\n",
    "#Extract just the target values for scoring\n",
    "publicPredsArr = publicPreds.loc[:,'Target']\n",
    "publicTarArr = publicTargets.loc[:,'EfficientlyCompletedBlockB']\n",
    "public10PredsArr = public10Preds.loc[:,'Target']\n",
    "public10TarArr = public10Targets.loc[:,'EfficientlyCompletedBlockB']\n",
    "public20PredsArr = public20Preds.loc[:,'Target']\n",
    "public20TarArr = public20Targets.loc[:,'EfficientlyCompletedBlockB']\n",
    "public30PredsArr = public30Preds.loc[:,'Target']\n",
    "public30TarArr = public30Targets.loc[:,'EfficientlyCompletedBlockB']\n",
    "\n",
    "print(\"Scoring on Public Leaderboard 10: \\n\")\n",
    "leaderBoard = scoreFunc(public10TarArr, public10PredsArr)\n",
    "print(\"Total: \", leaderBoard)\n",
    "\n",
    "print(\"\\nScoring on Public Leaderboard 20: \\n\")\n",
    "leaderBoard = scoreFunc(public20TarArr, public20PredsArr)\n",
    "print(\"Total: \", leaderBoard)\n",
    "\n",
    "print(\"\\nScoring on Public Leaderboard 30: \\n\")\n",
    "leaderBoard = scoreFunc(public30TarArr, public30PredsArr)\n",
    "print(\"Total: \", leaderBoard)\n",
    "\n",
    "print(\"\\nTotal Scoring on Public Leaderboard: \\n\")\n",
    "hiddenLeaderBoard = scoreFunc(publicTarArr,publicPredsArr)\n",
    "print(\"\\nTotal: \", hiddenLeaderBoard)\n",
    "\n",
    "\n",
    "################################################################################################################################\n",
    "#Final Leaderboard Scoring\n",
    "finalTargets = pd.read_csv('../EvaluationData/hidden_test.csv')\n",
    "\n",
    "\n",
    "#filter for final leaderboard ids\n",
    "list_of_ids = finalTargets['STUDENTID'].to_list()\n",
    "\n",
    "\n",
    "final10Preds = hidden10[hidden10.StudentID.isin(list_of_ids)]\n",
    "final10Targets = finalTargets[finalTargets.STUDENTID.isin(list_10_ids)]\n",
    "\n",
    "final20Preds = hidden20[hidden20.StudentID.isin(list_of_ids)]\n",
    "final20Targets = finalTargets[finalTargets.STUDENTID.isin(list_20_ids)]\n",
    "\n",
    "final30Preds = hidden30[hidden30.StudentID.isin(list_of_ids)]\n",
    "final30Targets = finalTargets[finalTargets.STUDENTID.isin(list_30_ids)]\n",
    "\n",
    "finalPreds = result[result.StudentID.isin(list_of_ids)]\n",
    "\n",
    "#Extract just the target values for scoring\n",
    "final10PredsArr = final10Preds.loc[:,'Target']\n",
    "final10TarArr = final10Targets.loc[:,'EfficientlyCompletedBlockB']\n",
    "final20PredsArr = final20Preds.loc[:,'Target']\n",
    "final20TarArr = final20Targets.loc[:,'EfficientlyCompletedBlockB']\n",
    "final30PredsArr = final30Preds.loc[:,'Target']\n",
    "final30TarArr = final30Targets.loc[:,'EfficientlyCompletedBlockB']\n",
    "finalPredsArr = finalPreds.loc[:,'Target']\n",
    "finalTarArr = finalTargets.loc[:,'EfficientlyCompletedBlockB']\n",
    "\n",
    "print(\"\\nScoring on final Leaderboard 10: \\n\")\n",
    "leaderBoard = scoreFunc(final10TarArr, final10PredsArr)\n",
    "print(\"Total: \", leaderBoard)\n",
    "\n",
    "print(\"\\nScoring on final Leaderboard 20: \\n\")\n",
    "leaderBoard = scoreFunc(final20TarArr, final20PredsArr)\n",
    "print(\"Total: \", leaderBoard)\n",
    "\n",
    "print(\"\\nScoring on final Leaderboard 30: \\n\")\n",
    "leaderBoard = scoreFunc(final30TarArr, final30PredsArr)\n",
    "print(\"Total: \", leaderBoard)\n",
    "\n",
    "print(\"\\nTotal Scoring on final Leaderboard: \\n\")\n",
    "finalLeaderBoard = scoreFunc(finalTarArr, finalPredsArr)\n",
    "print(\"Total: \", finalLeaderBoard)\n",
    "\n",
    "\n",
    "################################################################################################################################\n",
    "#Final Leaderboard Scoring\n",
    "combinedTargets = pd.concat([finalTargets,publicTargets])\n",
    "\n",
    "#Sort by Student ID to ensure same order\n",
    "combinedTargets = combinedTargets.sort_values(by='STUDENTID')\n",
    "result = result.sort_values(by='StudentID')\n",
    "\n",
    "combined10Targets = combinedTargets[combinedTargets.STUDENTID.isin(list_10_ids)]\n",
    "\n",
    "combined20Targets = combinedTargets[combinedTargets.STUDENTID.isin(list_20_ids)]\n",
    "\n",
    "combined30Targets = combinedTargets[combinedTargets.STUDENTID.isin(list_30_ids)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Extract just the target values for scoring\n",
    "combined10PredsArr = hidden10.loc[:,'Target']\n",
    "final10TarArr = combined10Targets.loc[:,'EfficientlyCompletedBlockB']\n",
    "combined20PredsArr = hidden20.loc[:,'Target']\n",
    "final20TarArr = combined20Targets.loc[:,'EfficientlyCompletedBlockB']\n",
    "combined30PredsArr = hidden30.loc[:,'Target']\n",
    "final30TarArr = combined30Targets.loc[:,'EfficientlyCompletedBlockB']\n",
    "\n",
    "resultPreds = result.loc[:,'Target']\n",
    "publicTarArr = combinedTargets.loc[:,'EfficientlyCompletedBlockB']\n",
    "\n",
    "print(\"\\nScoring on Combined Leaderboard 10: \\n\")\n",
    "leaderBoard = scoreFunc(final10TarArr, combined10PredsArr)\n",
    "print(\"Total: \", leaderBoard)\n",
    "\n",
    "print(\"\\nScoring on Combined Leaderboard 20: \\n\")\n",
    "leaderBoard = scoreFunc(final20TarArr, combined20PredsArr)\n",
    "print(\"Total: \", leaderBoard)\n",
    "\n",
    "print(\"\\nScoring on Combined Leaderboard 30: \\n\")\n",
    "leaderBoard = scoreFunc(final30TarArr, combined30PredsArr)\n",
    "print(\"Total: \", leaderBoard)\n",
    "\n",
    "print(\"\\nTotal Combined Scoring: \\n\")\n",
    "combinedLeaderBoard = scoreFunc(publicTarArr, resultPreds)\n",
    "print(\"Total: \", combinedLeaderBoard)\n",
    "\n",
    "print(\"\\nCombined Leaderboards: \", (finalLeaderBoard + hiddenLeaderBoard)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
